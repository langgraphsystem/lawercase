#!/usr/bin/env python3
"""
–¢–µ—Å—Ç –≤—Å–µ—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
"""

import asyncio
from core.llm_router import create_llm_router, LLMRequest, ModelType

async def test_all_providers():
    """–¢–µ—Å—Ç –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    print("üöÄ –¢–ï–°–¢ –í–°–ï–• LLM –ü–†–û–í–ê–ô–î–ï–†–û–í")
    print("=" * 50)

    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ –≤—Å–µ–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏
    config = {
        "providers": {
            "openai": {"enabled": True, "api_key": "demo-key"},
            "gemini": {"enabled": True, "api_key": "demo-key"},
            "claude": {"enabled": True, "api_key": "demo-key"},
            "mock": {"enabled": True, "failure_rate": 0.05}
        }
    }

    router = await create_llm_router(config)

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –æ—Ç–¥–µ–ª—å–Ω–æ
    providers_to_test = ["openai", "gemini", "claude", "mock"]

    for provider_name in providers_to_test:
        print(f"\nü§ñ === –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {provider_name.upper()} ===")

        request = LLMRequest(
            messages=[{"role": "user", "content": f"–ü—Ä–∏–≤–µ—Ç! –°–∫–∞–∂–∏ —á—Ç–æ —Ç—ã {provider_name}"}],
            model_type=ModelType.CHAT,
            max_tokens=100,
            provider_preference=[provider_name]  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤—ã–±–∏—Ä–∞–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        )

        response = await router.route_request(request)

        print(f"   –í—ã–±—Ä–∞–Ω–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {response.provider.value}")
        print(f"   –ú–æ–¥–µ–ª—å: {response.model}")
        print(f"   –£—Å–ø–µ—Ö: {'‚úÖ' if response.success else '‚ùå'}")
        print(f"   –û—Ç–≤–µ—Ç: {response.content}")
        print(f"   –¢–æ–∫–µ–Ω—ã: {response.tokens_used}")
        print(f"   –í—Ä–µ–º—è: {response.latency:.3f}s")
        print(f"   –°—Ç–æ–∏–º–æ—Å—Ç—å: ${response.cost:.6f}")

    # –¢–µ—Å—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–±–æ—Ä–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
    print(f"\nüéØ === –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ ===")
    auto_request = LLMRequest(
        messages=[{"role": "user", "content": "–ù–∞–ø–∏—à–∏ –∫—Ä–∞—Ç–∫–∏–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —Å–æ–≤–µ—Ç"}],
        model_type=ModelType.CHAT,
        max_tokens=100
    )

    auto_response = await router.route_request(auto_request)
    print(f"   –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω: {auto_response.provider.value}")
    print(f"   –ú–æ–¥–µ–ª—å: {auto_response.model}")
    print(f"   –û—Ç–≤–µ—Ç: {auto_response.content}")

    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìä === –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ===")
    stats = await router.get_providers_stats()
    for provider_name, provider_stats in stats.items():
        print(f"   {provider_name}:")
        print(f"     –ó–∞–ø—Ä–æ—Å–æ–≤: {provider_stats.total_requests}")
        print(f"     –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {provider_stats.success_rate:.1%}")
        print(f"     –°—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞: {provider_stats.average_latency:.3f}s")
        print(f"     –û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: ${provider_stats.total_cost:.6f}")

if __name__ == "__main__":
    asyncio.run(test_all_providers())