"""Knowledge base handlers for searching and managing reference documents.

Commands:
- /kb_search <query> - Semantic search in knowledge base
- /kb_stats - Show knowledge base statistics
"""

from __future__ import annotations

import structlog
from telegram import Update
from telegram.ext import CommandHandler, ContextTypes

from .context import BotContext

logger = structlog.get_logger(__name__)


def _bot_context(context: ContextTypes.DEFAULT_TYPE) -> BotContext:
    """Get BotContext from application bot_data."""
    return context.application.bot_data["bot_context"]


async def _is_authorized(bot_context: BotContext, update: Update) -> bool:
    """Check if user is authorized."""
    user_id = update.effective_user.id if update.effective_user else None
    if bot_context.is_authorized(user_id):
        return True
    if update.effective_message:
        await update.effective_message.reply_text("üö´ Access denied.")
    logger.warning("telegram.kb.unauthorized", user_id=user_id)
    return False


async def kb_search(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Search in knowledge base (reference documents only).

    Usage: /kb_search <query>
    Example: /kb_search EB-1A awards evidence
    """
    user_id = update.effective_user.id if update.effective_user else None
    logger.info("telegram.kb_search.received", user_id=user_id)

    bot_context = _bot_context(context)
    if not await _is_authorized(bot_context, update):
        return

    message = update.effective_message
    if not context.args:
        await message.reply_text(
            "‚ùå –£–∫–∞–∂–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.\n\n"
            "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: `/kb_search <–∑–∞–ø—Ä–æ—Å>`\n"
            "–ü—Ä–∏–º–µ—Ä: `/kb_search awards evidence`",
            parse_mode="Markdown",
        )
        return

    query = " ".join(context.args)
    logger.info("telegram.kb_search.query", user_id=user_id, query=query)

    await message.reply_text(f"üîç –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π: `{query}`\n‚è≥ –ò—â—É...", parse_mode="Markdown")

    try:
        from core.di.container import get_container

        container = get_container()
        memory = container.get("memory_manager")

        # Use dedicated knowledge base search
        results = await memory.aretrieve_knowledge_base(query=query, topk=5)

        if not results:
            await message.reply_text(
                "üì≠ –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.\n\n"
                "–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ –æ—Ç–ø—Ä–∞–≤–∫—É PDF —Ñ–∞–π–ª–∞ "
                '–∏ –≤—ã–±–æ—Ä "üìö –û–±—â–∏–µ –∑–Ω–∞–Ω–∏—è".'
            )
            logger.info("telegram.kb_search.no_results", user_id=user_id, query=query)
            return

        # Format results (plain text to avoid markdown parsing issues)
        lines = [
            "üìö –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π:",
            f"–ó–∞–ø—Ä–æ—Å: {query}",
            f"–ù–∞–π–¥–µ–Ω–æ: {len(results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
            "",
        ]

        for i, record in enumerate(results, 1):
            # Truncate text for display
            text_preview = record.text[:200] + "..." if len(record.text) > 200 else record.text
            text_preview = text_preview.replace("\n", " ").strip()

            # Get confidence/similarity score
            confidence = record.confidence or 0.0

            # Get tags
            tags = record.tags or []
            tags_str = ", ".join(tags[:3]) if tags else "–Ω–µ—Ç —Ç–µ–≥–æ–≤"

            lines.append(f"{i}. [{confidence:.0%} match]")
            lines.append(f"üìù {text_preview}")
            lines.append(f"üè∑Ô∏è –¢–µ–≥–∏: {tags_str}")
            if record.source:
                source_short = (
                    record.source[:30] + "..." if len(record.source) > 30 else record.source
                )
                lines.append(f"üìÑ –ò—Å—Ç–æ—á–Ω–∏–∫: {source_short}")
            lines.append("")

        response = "\n".join(lines)
        # Telegram message limit is 4096 chars
        if len(response) > 4000:
            response = response[:4000] + "\n\n... (–æ–±—Ä–µ–∑–∞–Ω–æ)"

        await message.reply_text(response, parse_mode=None)
        logger.info(
            "telegram.kb_search.success",
            user_id=user_id,
            query=query,
            results_count=len(results),
        )

    except Exception as e:
        logger.exception("telegram.kb_search.exception", user_id=user_id, error=str(e))
        await message.reply_text(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e!s}", parse_mode=None)


async def memory_search(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Search ALL semantic memory (KB + RFE + intake + everything).

    Usage: /memory_search <query>
    Example: /memory_search RFE denial reasons
    """
    user_id = update.effective_user.id if update.effective_user else None
    logger.info("telegram.memory_search.received", user_id=user_id)

    bot_context = _bot_context(context)
    if not await _is_authorized(bot_context, update):
        return

    message = update.effective_message
    if not context.args:
        await message.reply_text(
            "‚ùå –£–∫–∞–∂–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.\n\n"
            "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: `/memory_search <–∑–∞–ø—Ä–æ—Å>`\n"
            "–ü—Ä–∏–º–µ—Ä: `/memory_search RFE denial`\n\n"
            "–≠—Ç–∞ –∫–æ–º–∞–Ω–¥–∞ –∏—â–µ—Ç –ø–æ –í–°–ï–ô –ø–∞–º—è—Ç–∏ (KB + RFE + intake).",
            parse_mode="Markdown",
        )
        return

    query = " ".join(context.args)
    logger.info("telegram.memory_search.query", user_id=user_id, query=query)

    await message.reply_text(
        f"üîç –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–π –ø–∞–º—è—Ç–∏: `{query}`\n‚è≥ –ò—â—É...", parse_mode="Markdown"
    )

    try:
        from core.di.container import get_container

        container = get_container()
        memory = container.get("memory_manager")

        # Search ALL memory without filters
        results = await memory.semantic.aretrieve(
            query=query,
            user_id=None,  # Search across all users
            topk=5,
            filters=None,  # NO filters - search everything
        )

        if not results:
            await message.reply_text(
                "üì≠ –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –ø–∞–º—è—Ç–∏.\n\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã."
            )
            logger.info("telegram.memory_search.no_results", user_id=user_id, query=query)
            return

        # Format results (plain text to avoid markdown parsing issues)
        lines = [
            "üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –ø–æ –≤—Å–µ–π –ø–∞–º—è—Ç–∏:",
            f"–ó–∞–ø—Ä–æ—Å: {query}",
            f"–ù–∞–π–¥–µ–Ω–æ: {len(results)} –∑–∞–ø–∏—Å–µ–π",
            "",
        ]

        for i, record in enumerate(results, 1):
            # Truncate text for display
            text_preview = record.text[:200] + "..." if len(record.text) > 200 else record.text
            text_preview = text_preview.replace("\n", " ").strip()

            # Get confidence/similarity score
            confidence = record.confidence or 0.0

            # Get tags
            tags = record.tags or []
            tags_str = ", ".join(tags[:5]) if tags else "–Ω–µ—Ç —Ç–µ–≥–æ–≤"

            lines.append(f"{i}. [{confidence:.0%} match]")
            lines.append(f"üìù {text_preview}")
            lines.append(f"üè∑Ô∏è –¢–µ–≥–∏: {tags_str}")
            if record.source:
                source_short = (
                    record.source[:40] + "..." if len(record.source) > 40 else record.source
                )
                lines.append(f"üìÑ –ò—Å—Ç–æ—á–Ω–∏–∫: {source_short}")
            lines.append("")

        response = "\n".join(lines)
        # Telegram message limit is 4096 chars
        if len(response) > 4000:
            response = response[:4000] + "\n\n... (–æ–±—Ä–µ–∑–∞–Ω–æ)"

        await message.reply_text(response, parse_mode=None)
        logger.info(
            "telegram.memory_search.success",
            user_id=user_id,
            query=query,
            results_count=len(results),
        )

    except Exception as e:
        logger.exception("telegram.memory_search.exception", user_id=user_id, error=str(e))
        await message.reply_text(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e!s}", parse_mode=None)


async def memory_stats(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Show complete memory statistics with tag breakdown.

    Usage: /memory_stats
    """
    user_id = update.effective_user.id if update.effective_user else None
    logger.info("telegram.memory_stats.received", user_id=user_id)

    bot_context = _bot_context(context)
    if not await _is_authorized(bot_context, update):
        return

    message = update.effective_message
    await message.reply_text("üìä –ó–∞–≥—Ä—É–∂–∞—é –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏...")

    try:
        from core.di.container import get_container

        container = get_container()
        memory = container.get("memory_manager")

        # Get semantic store for direct stats
        semantic_store = memory.semantic

        # Count total records
        total_count = await semantic_store.acount()

        # Count by different tags
        kb_count = await semantic_store.acount_by_tags(["knowledge_base"])
        case_doc_count = await semantic_store.acount_by_tags(["case_document"])

        # Count other categories
        other_count = total_count - kb_count - case_doc_count

        # Get all unique sources
        all_sources = await semantic_store.aget_unique_sources()

        # Format response (plain text to avoid markdown parsing issues)
        lines = [
            "üìä –ü–æ–ª–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏:",
            "",
            f"üìÅ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total_count}",
            "",
            "üìÇ –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:",
            f"   üìö –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π (knowledge_base): {kb_count}",
            f"   üìÅ –î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∫–µ–π—Å–∞–º (case_document): {case_doc_count}",
            f"   üìã –î—Ä—É–≥–∏–µ –∑–∞–ø–∏—Å–∏ (RFE, intake –∏ —Ç.–¥.): {other_count}",
            "",
        ]

        if all_sources:
            lines.append("üìÑ –í—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –ø–∞–º—è—Ç–∏:")
            for src in all_sources[:15]:
                source_name = src["source"] or "unknown"
                # Shorten long filenames
                if len(source_name) > 50:
                    source_name = source_name[:47] + "..."
                lines.append(f"   ‚Ä¢ {source_name}: {src['count']} –∑–∞–ø–∏—Å–µ–π")
            if len(all_sources) > 15:
                lines.append(f"   ... –∏ –µ—â—ë {len(all_sources) - 15} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
            lines.append("")

        lines.append("üí° –ö–æ–º–∞–Ω–¥—ã –ø–æ–∏—Å–∫–∞:")
        lines.append("   /kb_search <–∑–∞–ø—Ä–æ—Å> - —Ç–æ–ª—å–∫–æ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π")
        lines.append("   /memory_search <–∑–∞–ø—Ä–æ—Å> - –í–°–Ø –ø–∞–º—è—Ç—å (–≤–∫–ª—é—á–∞—è RFE)")

        response = "\n".join(lines)
        await message.reply_text(response, parse_mode=None)

        logger.info(
            "telegram.memory_stats.success",
            user_id=user_id,
            total_count=total_count,
            kb_count=kb_count,
            case_doc_count=case_doc_count,
            other_count=other_count,
            sources_count=len(all_sources),
        )

    except Exception as e:
        logger.exception("telegram.memory_stats.exception", user_id=user_id, error=str(e))
        await message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {e!s}", parse_mode=None)


async def kb_stats(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Show knowledge base statistics.

    Usage: /kb_stats
    """
    user_id = update.effective_user.id if update.effective_user else None
    logger.info("telegram.kb_stats.received", user_id=user_id)

    bot_context = _bot_context(context)
    if not await _is_authorized(bot_context, update):
        return

    message = update.effective_message
    await message.reply_text("üìä –ó–∞–≥—Ä—É–∂–∞—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")

    try:
        from core.di.container import get_container

        container = get_container()
        memory = container.get("memory_manager")

        # Get semantic store for direct stats
        semantic_store = memory.semantic

        # Count total records
        total_count = await semantic_store.acount()

        # Count knowledge_base records specifically
        kb_count = await semantic_store.acount_by_tags(["knowledge_base"])

        # Count case documents
        case_doc_count = await semantic_store.acount_by_tags(["case_document"])

        # Get sources for knowledge base documents
        kb_sources = await semantic_store.aget_unique_sources(tags=["knowledge_base"])

        # Format response (plain text to avoid markdown parsing issues)
        lines = [
            "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:",
            "",
            f"üìÅ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total_count}",
            f"üìö –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π (knowledge_base): {kb_count}",
            f"üìÇ –î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∫–µ–π—Å–∞–º (case_document): {case_doc_count}",
            "",
        ]

        if kb_sources:
            lines.append("üìÑ –î–æ–∫—É–º–µ–Ω—Ç—ã –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π:")
            for src in kb_sources[:10]:
                source_name = src["source"] or "unknown"
                # Shorten long filenames
                if len(source_name) > 40:
                    source_name = source_name[:37] + "..."
                lines.append(f"  ‚Ä¢ {source_name}: {src['count']} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            lines.append("")

        if kb_count > 0:
            lines.append("‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –¥–æ—Å—Ç—É–ø–Ω–∞ –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã.")
            lines.append("")
            lines.append("üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /kb_search <–∑–∞–ø—Ä–æ—Å> –¥–ª—è –ø–æ–∏—Å–∫–∞.")
        else:
            lines.append("‚ö†Ô∏è –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞.")
            lines.append("")
            lines.append("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:")
            lines.append("1. –û—Ç–ø—Ä–∞–≤—å—Ç–µ PDF —Ñ–∞–π–ª")
            lines.append('2. –í—ã–±–µ—Ä–∏—Ç–µ "üìö –û–±—â–∏–µ –∑–Ω–∞–Ω–∏—è"')

        response = "\n".join(lines)
        await message.reply_text(response, parse_mode=None)

        logger.info(
            "telegram.kb_stats.success",
            user_id=user_id,
            total_count=total_count,
            kb_count=kb_count,
            case_doc_count=case_doc_count,
            sources_count=len(kb_sources),
        )

    except Exception as e:
        logger.exception("telegram.kb_stats.exception", user_id=user_id, error=str(e))
        await message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {e!s}", parse_mode=None)


def get_handlers(bot_context: BotContext) -> list:
    """Return knowledge-base and memory-related handlers."""
    return [
        CommandHandler("kb_search", kb_search),
        CommandHandler("kb_stats", kb_stats),
        CommandHandler("memory_search", memory_search),
        CommandHandler("memory_stats", memory_stats),
    ]
